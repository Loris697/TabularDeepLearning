{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4eff76f-7139-48c9-bf0f-d9a28715c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "\n",
    "import sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e186dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress only ConvergenceWarnings\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdace7ad-fcd7-40c9-895f-1bdf20a7fe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 47 -> tae 48 \n",
    "#2079 -> eucalyptus 188\n",
    "#3902 -> pc4 1049\n",
    "# 3561 -> profb 470\n",
    "dataset_ids  = [1483, 180, 1459, 1509, 4538]\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffbc9c72-39eb-4032-a64c-ba0989617e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grids for each model\n",
    "param_grid = {\n",
    "    \"Logistic Regression\": {\n",
    "        'model__C': [0.1, 1, 10],\n",
    "        'model__solver': ['lbfgs', 'liblinear']\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__max_features': [None, 'sqrt', 'log2']\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        'model__C': [0.1, 1, 10],\n",
    "        'model__kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    },\n",
    "    \"MLP\":{\n",
    "        #'model__hidden_layer_sizes': [(50,), (100,), (50,50), (100,100)],\n",
    "        'model__hidden_layer_sizes': [(50,), (100,100)],\n",
    "        'model__activation': ['tanh', 'relu'],\n",
    "        'model__solver': ['sgd', 'adam'],\n",
    "        'model__alpha': [0.0001, 0.05],\n",
    "        #'model__batch_size': [64, 128, 256],\n",
    "        #'model__learning_rate': ['constant','adaptive'],\n",
    "        'model__learning_rate_init': [0.001, 0.01],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5a00c61-db22-4163-81c2-34aa937a72a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenML Dataset\n",
      "==============\n",
      "Name..........: ldpa\n",
      "Version.......: 1\n",
      "Format........: ARFF\n",
      "Upload Date...: 2015-05-22 23:34:22\n",
      "Licence.......: Public\n",
      "Download URL..: https://api.openml.org/data/v1/download/1590940/ldpa.arff\n",
      "OpenML URL....: https://www.openml.org/d/1483\n",
      "# of features.: 8\n",
      "# of instances: 164860\n",
      "OpenML Dataset\n",
      "==============\n",
      "Name..........: covertype\n",
      "Version.......: 1\n",
      "Format........: ARFF\n",
      "Upload Date...: 2014-04-23 13:14:37\n",
      "Licence.......: Public\n",
      "Download URL..: https://api.openml.org/data/v1/download/3615/covertype.arff\n",
      "OpenML URL....: https://www.openml.org/d/180\n",
      "# of features.: 55\n",
      "# of instances: 110393\n",
      "OpenML Dataset\n",
      "==============\n",
      "Name..........: artificial-characters\n",
      "Version.......: 1\n",
      "Format........: ARFF\n",
      "Upload Date...: 2015-05-21 20:58:53\n",
      "Licence.......: Public\n",
      "Download URL..: https://api.openml.org/data/v1/download/1586212/artificial-characters.arff\n",
      "OpenML URL....: https://www.openml.org/d/1459\n",
      "# of features.: 8\n",
      "# of instances: 10218\n",
      "OpenML Dataset\n",
      "==============\n",
      "Name..........: walking-activity\n",
      "Version.......: 1\n",
      "Format........: ARFF\n",
      "Upload Date...: 2015-05-26 16:14:38\n",
      "Licence.......: Public\n",
      "Download URL..: https://api.openml.org/data/v1/download/1592317/walking-activity.arff\n",
      "OpenML URL....: https://www.openml.org/d/1509\n",
      "# of features.: 5\n",
      "# of instances: 149332\n",
      "OpenML Dataset\n",
      "==============\n",
      "Name..........: GesturePhaseSegmentationProcessed\n",
      "Version.......: 1\n",
      "Format........: ARFF\n",
      "Upload Date...: 2016-02-17 11:42:33\n",
      "Licence.......: Public\n",
      "Download URL..: https://api.openml.org/data/v1/download/1798765/GesturePhaseSegmentationProcessed.arff\n",
      "OpenML URL....: https://www.openml.org/d/4538\n",
      "# of features.: 33\n",
      "# of instances: 9873\n"
     ]
    }
   ],
   "source": [
    "for dataset_id in dataset_ids:\n",
    "    # Load the dataset\n",
    "    dataset = openml.datasets.get_dataset(dataset_id, \n",
    "                                          download_data = True, \n",
    "                                          download_qualities =True, \n",
    "                                          download_features_meta_data=True\n",
    "                                         )\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17a7215a-773c-4887-9532-54104ca98431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ldpa\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/loriscino/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/loriscino/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/loriscino/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/loriscino/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/loriscino/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/loriscino/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/loriscino/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/loriscino/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/loriscino/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/loriscino/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/loriscino/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/loriscino/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/loriscino/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/loriscino/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/loriscino/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV] END ..................model__C=0.1, model__solver=lbfgs; total time=  10.6s\n",
      "[CV] END ..............model__C=0.1, model__solver=liblinear; total time=  11.4s\n",
      "[CV] END ................model__C=1, model__solver=liblinear; total time=  11.3s\n",
      "[CV] END ...............model__C=10, model__solver=liblinear; total time=   8.1s\n",
      "[CV] END ..model__max_features=None, model__n_estimators=100; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/loriscino/opt/anaconda3/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ..................model__C=0.1, model__solver=lbfgs; total time=  11.2s\n",
      "[CV] END ....................model__C=1, model__solver=lbfgs; total time=  10.9s\n",
      "[CV] END ................model__C=1, model__solver=liblinear; total time=  11.3s\n",
      "[CV] END ...............model__C=10, model__solver=liblinear; total time=   8.1s\n",
      "[CV] END ..model__max_features=None, model__n_estimators=100; total time= 1.6min\n",
      "[CV] END ..............model__C=0.1, model__solver=liblinear; total time=  11.6s\n",
      "[CV] END ....................model__C=1, model__solver=lbfgs; total time=  10.6s\n",
      "[CV] END ................model__C=1, model__solver=liblinear; total time=  11.3s\n",
      "[CV] END ..model__max_features=None, model__n_estimators=100; total time= 1.7min\n",
      "[CV] END ..................model__C=0.1, model__solver=lbfgs; total time=  10.8s\n",
      "[CV] END ..............model__C=0.1, model__solver=liblinear; total time=  11.3s\n",
      "[CV] END ...................model__C=10, model__solver=lbfgs; total time=  10.8s\n",
      "[CV] END ...............model__C=10, model__solver=liblinear; total time=   8.3s\n",
      "[CV] END ...model__max_features=None, model__n_estimators=10; total time=   9.8s\n",
      "[CV] END ..model__max_features=None, model__n_estimators=100; total time= 1.7min\n",
      "[CV] END ..............model__C=0.1, model__solver=liblinear; total time=  11.6s\n",
      "[CV] END ................model__C=1, model__solver=liblinear; total time=  11.5s\n",
      "[CV] END ...................model__C=10, model__solver=lbfgs; total time=  10.4s\n",
      "[CV] END ...model__max_features=None, model__n_estimators=10; total time=   9.8s\n",
      "[CV] END ..model__max_features=None, model__n_estimators=100; total time= 1.7min\n",
      "[CV] END ...model__max_features=sqrt, model__n_estimators=10; total time=   3.6s\n",
      "[CV] END ...model__max_features=sqrt, model__n_estimators=10; total time=   3.8s\n",
      "[CV] END ...model__max_features=sqrt, model__n_estimators=10; total time=   3.7s\n",
      "[CV] END ..model__max_features=sqrt, model__n_estimators=100; total time=  33.0s\n",
      "[CV] END ...model__max_features=sqrt, model__n_estimators=10; total time=   3.7s\n",
      "[CV] END ..model__max_features=sqrt, model__n_estimators=100; total time=  33.6s\n",
      "[CV] END ...model__max_features=sqrt, model__n_estimators=10; total time=   3.6s\n",
      "[CV] END ..model__max_features=sqrt, model__n_estimators=100; total time=  33.6s\n",
      "[CV] END ..model__max_features=sqrt, model__n_estimators=100; total time=  33.3s\n",
      "[CV] END .model__max_features=sqrt, model__n_estimators=1000; total time= 5.6min\n",
      "[CV] END ..model__max_features=sqrt, model__n_estimators=100; total time=  33.7s\n",
      "[CV] END .model__max_features=sqrt, model__n_estimators=1000; total time= 5.6min\n",
      "[CV] END ...model__max_features=log2, model__n_estimators=10; total time=   3.6s\n",
      "[CV] END ...model__max_features=log2, model__n_estimators=10; total time=   3.6s\n",
      "[CV] END ...model__max_features=log2, model__n_estimators=10; total time=   3.6s\n",
      "[CV] END ...model__max_features=log2, model__n_estimators=10; total time=   3.7s\n",
      "[CV] END ...model__max_features=log2, model__n_estimators=10; total time=   3.7s\n",
      "[CV] END ..model__max_features=log2, model__n_estimators=100; total time=  33.1s\n",
      "[CV] END .model__max_features=sqrt, model__n_estimators=1000; total time= 5.6min\n",
      "[CV] END .model__max_features=sqrt, model__n_estimators=1000; total time= 5.6min\n",
      "[CV] END ..............model__C=0.1, model__solver=liblinear; total time=  11.4s\n",
      "[CV] END ....................model__C=1, model__solver=lbfgs; total time=  11.1s\n",
      "[CV] END ...................model__C=10, model__solver=lbfgs; total time=  10.2s\n",
      "[CV] END ...................model__C=10, model__solver=lbfgs; total time=   7.8s\n",
      "[CV] END ...model__max_features=None, model__n_estimators=10; total time=  10.0s\n",
      "[CV] END .model__max_features=None, model__n_estimators=1000; total time=16.7min\n",
      "[CV] END ..................model__C=0.1, model__solver=lbfgs; total time=  11.1s\n",
      "[CV] END ....................model__C=1, model__solver=lbfgs; total time=  10.9s\n",
      "[CV] END ................model__C=1, model__solver=liblinear; total time=  11.0s\n",
      "[CV] END ...............model__C=10, model__solver=liblinear; total time=   8.2s\n",
      "[CV] END ...model__max_features=None, model__n_estimators=10; total time=  10.1s\n",
      "[CV] END .model__max_features=None, model__n_estimators=1000; total time=16.7min\n",
      "[CV] END ..................model__C=0.1, model__solver=lbfgs; total time=  10.9s\n",
      "[CV] END ....................model__C=1, model__solver=lbfgs; total time=  11.2s\n",
      "[CV] END ...................model__C=10, model__solver=lbfgs; total time=  10.9s\n",
      "[CV] END ...............model__C=10, model__solver=liblinear; total time=   8.4s\n",
      "[CV] END ...model__max_features=None, model__n_estimators=10; total time=  10.0s\n",
      "[CV] END .model__max_features=None, model__n_estimators=1000; total time=16.7min\n",
      "[CV] END ..model__max_features=log2, model__n_estimators=100; total time=  33.5s\n",
      "[CV] END ..model__max_features=log2, model__n_estimators=100; total time=  32.8s\n",
      "[CV] END ..model__max_features=log2, model__n_estimators=100; total time=  33.3s\n",
      "[CV] END ..model__max_features=log2, model__n_estimators=100; total time=  33.4s\n",
      "[CV] END .model__max_features=log2, model__n_estimators=1000; total time= 5.6min\n",
      "[CV] END .model__max_features=sqrt, model__n_estimators=1000; total time= 5.6min\n",
      "[CV] END .model__max_features=log2, model__n_estimators=1000; total time= 4.8min\n",
      "[CV] END .model__max_features=None, model__n_estimators=1000; total time=16.7min\n",
      "[CV] END .model__max_features=None, model__n_estimators=1000; total time=16.7min\n",
      "[CV] END .model__max_features=log2, model__n_estimators=1000; total time= 4.0min\n",
      "[CV] END .model__max_features=log2, model__n_estimators=1000; total time= 4.0min\n",
      "[CV] END .model__max_features=log2, model__n_estimators=1000; total time= 5.0min\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Setup the GridSearchCV pipeline\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(model, \n\u001b[1;32m     63\u001b[0m                                param_grid\u001b[38;5;241m=\u001b[39mparam_grid[model_name],\n\u001b[1;32m     64\u001b[0m                                n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m                                verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     69\u001b[0m                               )\n\u001b[0;32m---> 70\u001b[0m     \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m         model_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(grid_search\u001b[38;5;241m.\u001b[39mcv_results_)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    885\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    887\u001b[0m     )\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 891\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    895\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1391\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1392\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    832\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    835\u001b[0m         )\n\u001b[1;32m    836\u001b[0m     )\n\u001b[0;32m--> 838\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    860\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 935\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop through each dataset ID\n",
    "dataset_results = None\n",
    "\n",
    "for dataset_id in dataset_ids:\n",
    "    # Load the dataset\n",
    "    dataset = openml.datasets.get_dataset(dataset_id, \n",
    "                                          download_data = True, \n",
    "                                          download_qualities =True, \n",
    "                                          download_features_meta_data=True\n",
    "                                         )\n",
    "    print(dataset.name)\n",
    "    \n",
    "    X, y, _, attribute_names = dataset.get_data(target=dataset.default_target_attribute)\n",
    "    \n",
    "    # Create train-test splits\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "    numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist() \n",
    "\n",
    "    # Create transformers for the numerical and categorical data\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with the mean\n",
    "        ('scaler', StandardScaler())  # Scale data\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with the most frequent value\n",
    "        ('oe', OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value = -1)) \n",
    "    ])\n",
    "\n",
    "    # Combine transformers into a ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ])\n",
    "    # Define models to test\n",
    "    models = {\n",
    "        \"Logistic Regression\": Pipeline(steps=[\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('model', LogisticRegression())\n",
    "            ]),\n",
    "        \"Random Forest\": Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', RandomForestClassifier())\n",
    "        ]),\n",
    "        \"SVM\": Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', SVC())\n",
    "        ]),\n",
    "        \"MLP\": Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', MLPClassifier())\n",
    "        ])\n",
    "    }\n",
    "    model_results = None\n",
    "    \n",
    "    # Loop through each model\n",
    "    for model_name, model in models.items():\n",
    "        # Setup the GridSearchCV pipeline\n",
    "        grid_search = GridSearchCV(model, \n",
    "                                   param_grid=param_grid[model_name],\n",
    "                                   n_jobs=-1,\n",
    "                                   cv=5, \n",
    "                                   scoring='accuracy', \n",
    "                                   return_train_score=True,\n",
    "                                   verbose=2\n",
    "                                  )\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        if model_results is None:\n",
    "            model_results = pd.DataFrame(grid_search.cv_results_)\n",
    "            model_results[\"Model\"] = model_name\n",
    "        else:\n",
    "            dataframe = pd.DataFrame(grid_search.cv_results_)\n",
    "            dataframe[\"Model\"] = model_name\n",
    "            model_results = pd.concat([model_results, dataframe], axis = 0)\n",
    "        \n",
    "        # Predict the test set\n",
    "        y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "        \n",
    "        # Calculate the accuracy\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Store the results\n",
    "        results.append({\n",
    "            'dataset_id': dataset_id,\n",
    "            'dataset_name': dataset.name,\n",
    "            'model': model_name,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'accuracy': accuracy\n",
    "        })\n",
    "\n",
    "    if dataset_results is None:\n",
    "        dataset_results = model_results\n",
    "        dataset_results[\"Dataset\"] = dataset.name\n",
    "    else:\n",
    "        dataframe = model_results\n",
    "        dataframe[\"Dataset\"] = dataset.name\n",
    "        dataset_results = pd.concat([dataset_results, dataframe], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435d89e-21e7-4929-b822-909b677b3a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3325c9d9-a0fa-4694-9746-2f67b8d22f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    print(f\"Dataset ID: {result['dataset_id']} - {result['dataset_name']}\\n\"\n",
    "          f\"Model: {result['model']}\\nBest Params: {result['best_params']}\\n\"\n",
    "          f\"Accuracy: {result['accuracy']:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2916e8-7567-4ce0-b52a-a448f55b24a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_id in dataset_ids:\n",
    "    dataset = openml.datasets.get_dataset(dataset_id, \n",
    "                                          download_data = True, \n",
    "                                          download_qualities =True, \n",
    "                                          download_features_meta_data=True\n",
    "                                         )\n",
    "    print(dataset.name)\n",
    "    \n",
    "    X, y, categorical_indicator, attribute_names = dataset.get_data(target=dataset.default_target_attribute)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "    # Identify categorical and numerical columns\n",
    "    numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # Preprocessing for numerical data\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())])\n",
    "\n",
    "    # Preprocessing for categorical data\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('oe', OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value = -1))])\n",
    "\n",
    "    # Bundle preprocessing for numerical and categorical data\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)])\n",
    "\n",
    "    # Define the model\n",
    "    model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                            ('classifier', RandomForestClassifier(random_state=42))])\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Get feature importances\n",
    "    if hasattr(model.named_steps['classifier'], 'feature_importances_'):\n",
    "        importances = model.named_steps['classifier'].feature_importances_\n",
    "    else:\n",
    "        importances = model.named_steps['classifier'].coef_[0]\n",
    "    \n",
    "    # Get feature names after preprocessing\n",
    "    feature_names = numerical_cols + categorical_cols\n",
    "    \n",
    "    # Summarize feature importances\n",
    "    forest_importances = pd.Series(importances, index=feature_names)\n",
    "    \n",
    "    # Sort the feature importances in descending order and select the top 10\n",
    "    forest_importances = forest_importances.sort_values(ascending=False)[:10]\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    forest_importances.plot.bar(ax=ax)\n",
    "    ax.set_title(\"Top 10 Feature Importances Using Random Forest\")\n",
    "    ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "    ax.set_xlabel(\"Features\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Sort features and select the top two\n",
    "    top_two_features = forest_importances.nlargest(2).index.tolist()\n",
    "\n",
    "    # Create a DataFrame for plotting\n",
    "    transformed_data = pd.DataFrame(model.named_steps['preprocessor'].transform(X_test), \n",
    "                                    columns=feature_names)\n",
    "    transformed_data['target'] = y_test.to_numpy()\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for class_value in np.unique(transformed_data['target']):\n",
    "        plt_data = transformed_data[transformed_data['target'] == class_value]\n",
    "        plt.scatter(plt_data[top_two_features[0]], plt_data[top_two_features[1]], \n",
    "                    alpha=0.5, label=f'Class {class_value}')\n",
    "\n",
    "    plt.xlabel(top_two_features[0])\n",
    "    plt.ylabel(top_two_features[1])\n",
    "    plt.title(f\"Scatter Plot of Top Two Features for Dataset {dataset_id}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14df92b-7333-44f3-8393-2a91a1199319",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
